# AI-security-engineer-take-home

Instalar dependências
```
poetry install
```

Executar o projeto
```
poetry run python3 src/main.py
```
___

## Refs

**Workspace**
[Poetry Docs](https://python-poetry.org/docs/basic-usage/)


__

[Mitre Atlas](https://atlas.mitre.org/matrices/ATLAS)

[LLM AI Cybersecurity & Governance Checklist](https://owasp.org/www-project-top-10-for-large-language-model-applications/llm-top-10-governance-doc/LLM_AI_Security_and_Governance_Checklist-v1.pdf)

[Artificial Intelligence Risk Management Framework (AI RMF 1.0)](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf)

___


**Papers**

[ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/html/2402.11753v2)

[Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173)

[Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications](https://arxiv.org/abs/2401.07612)

[How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/html/2401.06373v2)

[Assessing Prompt Injection Risks in 200+ Custom GPTs](https://arxiv.org/abs/2311.11538)

[Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles](https://arxiv.org/abs/2311.14876)

[A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models](https://arxiv.org/abs/2312.10982)

[Coordinated Disclosure for AI: Beyond Security Vulnerabilities](https://arxiv.org/abs/2402.07039)

[Review of Generative AI Methods in Cybersecurity](https://arxiv.org/html/2403.08701v2)

[LLM BugSWAT 2023](https://www.landh.tech/blog/20240304-google-hack-50000/)

[Hacking Google Bard - From Prompt Injection to Data Exfiltration](https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/)

[Roses are red, grapes make fine wine. I stole your password, now it's mine](https://hiddenlayer.com/research/new-google-gemini-content-manipulation-vulns-found/)

[jailbreak-classification](https://huggingface.co/datasets/jackhhao/jailbreak-classification?row=95)

[Real World LLM Exploits](https://www.lakera.ai/ai-security-guides/real-world-llm-exploits)

[Compromising LLMs using Indirect Prompt Injection](https://github.com/greshake/llm-security)

[ComPromptMized: Unleashing Zero-click Worms that Target GenAI-Powered Applications](https://sites.google.com/view/compromptmized)

___


[Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers’ Coding Practices with Insecure Suggestions from Poisoned AI Models](https://arxiv.org/html/2312.06227v1)

[Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](https://arxiv.org/abs/2311.09433)

[Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems](https://arxiv.org/abs/2311.11796)

___

[AI Village](https://aivillage.org/blog/)

[AI Incident Database](https://incidentdatabase.ai/)

[OECD AI Incidents Monitor](https://oecd.ai/en/incidents)

[Google's AI Red Team: the ethical hackers making AI safer](https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/)

[Microsoft AI Red Team](https://learn.microsoft.com/en-us/security/ai-red-team/)

___