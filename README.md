# AI-security-engineer-take-home

Instalar dependências
```
poetry install
```

Executar o projeto
```
poetry run python3 src/main.py
```
___

## Refs

**Workspace**
[Poetry Docs](https://python-poetry.org/docs/basic-usage/)

**Papers**

[ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/html/2402.11753v2)

[Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173)

[Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications](https://arxiv.org/abs/2401.07612)

[How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/html/2401.06373v2)

[Assessing Prompt Injection Risks in 200+ Custom GPTs](https://arxiv.org/abs/2311.11538)

[Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles](https://arxiv.org/abs/2311.14876)

[A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models](https://arxiv.org/abs/2312.10982)

[Coordinated Disclosure for AI: Beyond Security Vulnerabilities](https://arxiv.org/abs/2402.07039)

[Review of Generative AI Methods in Cybersecurity](https://arxiv.org/html/2403.08701v2)

[LLM BugSWAT 2023](https://www.landh.tech/blog/20240304-google-hack-50000/)

[Hacking Google Bard - From Prompt Injection to Data Exfiltration](https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/)

[Roses are red, grapes make fine wine. I stole your password, now it's mine](https://hiddenlayer.com/research/new-google-gemini-content-manipulation-vulns-found/)

[jailbreak-classification](https://huggingface.co/datasets/jackhhao/jailbreak-classification?row=95)

[Real World LLM Exploits](https://www.lakera.ai/ai-security-guides/real-world-llm-exploits)



[Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers’ Coding Practices with Insecure Suggestions from Poisoned AI Models](https://arxiv.org/html/2312.06227v1)

[Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](https://arxiv.org/abs/2311.09433)

[Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems](https://arxiv.org/abs/2311.11796)